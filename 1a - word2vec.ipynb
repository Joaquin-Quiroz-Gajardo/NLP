{"cells":[{"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Word2vect\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kCED1hh-Ioyf"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PUbfVnzIIoMj"},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RIO7b8GjAC17"},"outputs":[],"source":["corpus = np.array(['que dia es hoy', 'martes el dia de hoy es martes', 'martes muchas gracias'])"]},{"cell_type":"markdown","metadata":{"id":"8WqdaTmO8P1r"},"source":["Documento 1 --> que dia es hoy \\\n","Documento 2 --> martes el dia de hoy es martes \\\n","Documento 3 --> martes muchas gracias"]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 1 - Obtener el vocabulario del corpus (los términos utilizados)\n","- Cada documento transformarlo en una lista de términos\n","- Armar un vector de términos no repetidos de todos los documentos"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"3ZqTOZzDI7uv"},"outputs":[{"name":"stdout","output_type":"stream","text":["[['que', 'dia', 'es', 'hoy'], ['martes', 'el', 'dia', 'de', 'hoy', 'es', 'martes'], ['martes', 'muchas', 'gracias']]\n","['de' 'dia' 'el' 'es' 'gracias' 'hoy' 'martes' 'muchas' 'que']\n"]}],"source":["new_corpus = []\n","for i in corpus:\n","    terms = i.split()\n","    new_corpus.append(terms)\n","print(new_corpus)\n","\n","unique_corpus = np.hstack(new_corpus)\n","unique_corpus = np.unique(unique_corpus)\n","print(unique_corpus)"]},{"cell_type":"markdown","metadata":{"id":"RUhH983FI7It"},"source":["### 2- OneHot encoding\n","Data una lista de textos, devolver una matriz con la representación oneHotEncoding de estos"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Os0AAQo6I6Z1"},"outputs":[{"data":{"text/plain":["array([[0., 1., 0., 1., 0., 1., 0., 0., 1.],\n","       [1., 1., 1., 1., 0., 1., 1., 0., 0.],\n","       [0., 0., 0., 0., 1., 0., 1., 1., 0.]])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["one_hot_encoding = np.zeros((len(new_corpus), unique_corpus.size))\n","for document_index, document in enumerate(new_corpus):\n","    for token in document:\n","        if token in unique_corpus:\n","            token_index = np.where(unique_corpus == token)\n","            one_hot_encoding[document_index, token_index] = 1\n","one_hot_encoding\n"]},{"cell_type":"markdown","metadata":{"id":"IIyWGmCpJVQL"},"source":["### 3- Vectores de frecuencia\n","Data una lista de textos, devolver una matriz con la representación de frecuencia de estos"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"yqij_7eHJbUi"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 1. 0. 1. 0. 1. 0. 0. 1.]\n"," [1. 1. 1. 1. 0. 1. 2. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 1. 1. 0.]]\n"]}],"source":["vector_frecuency = np.zeros((len(new_corpus), unique_corpus.size))\n","for document_index, document in enumerate(new_corpus):\n","    unique, counts = np.unique(document, return_counts=True)\n","    for index, value in enumerate(unique):\n","        index_term = np.where(unique_corpus == value)\n","        particular_term_frecuency=counts[index]\n","        vector_frecuency[document_index, index_term] = particular_term_frecuency\n","print(vector_frecuency)"]},{"cell_type":"markdown","metadata":{"id":"z_Ot8HvWJcBu"},"source":["### 4- TF-IDF\n","Data una lista de textos, devolver una matriz con la representacion TFIDF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["term_frecuency = np.zeros((len(new_corpus), unique_corpus.size))\n","for document_index, document in enumerate(new_corpus):\n","    unique, counts = np.unique(document, return_counts=True)\n","    for index, value in enumerate(unique):\n","        index_term = np.where(unique_corpus == value)\n","        particular_term_frecuency=counts[index]/len(document)\n","        term_frecuency[document_index, index_term] = particular_term_frecuency\n","print(term_frecuency)"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"waG_oWtpJjRw"},"outputs":[{"data":{"text/plain":["array([0.47712125, 0.17609126, 0.47712125, 0.17609126, 0.47712125,\n","       0.17609126, 0.17609126, 0.47712125, 0.47712125])"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["inverse_document_frecuency=np.sum(one_hot_encoding, axis=0)\n","inverse_document_frecuency=len(new_corpus)/inverse_document_frecuency\n","inverse_document_frecuency=np.log10(inverse_document_frecuency)\n","inverse_document_frecuency"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0.        , 0.04402281, 0.        , 0.04402281, 0.        ,\n","        0.04402281, 0.        , 0.        , 0.11928031],\n","       [0.06816018, 0.02515589, 0.06816018, 0.02515589, 0.        ,\n","        0.02515589, 0.05031179, 0.        , 0.        ],\n","       [0.        , 0.        , 0.        , 0.        , 0.15904042,\n","        0.        , 0.05869709, 0.15904042, 0.        ]])"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["tf_idf=term_frecuency*inverse_document_frecuency\n","tf_idf"]},{"cell_type":"markdown","metadata":{"id":"xMcsfndWJjm_"},"source":["### 5 - Comparación de documentos\n","Realizar una funcion que reciba el corpus y el índice de un documento y devuelva los documentos ordenados por la similitud coseno"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"CZdiop6IJpZN"},"outputs":[{"name":"stdout","output_type":"stream","text":["the cosine similarity between document 1 and 2 is: 0.200341902680987\n","the cosine similarity between document 1 and 3 is: 0.0\n","the cosine similarity between document 2 and 3 is: 0.10845711727883087\n"]}],"source":["print(\"the cosine similarity between document 1 and 2 is:\",cosine_similarity(tf_idf[0],tf_idf[1]))\n","print(\"the cosine similarity between document 1 and 3 is:\",cosine_similarity(tf_idf[0],tf_idf[2]))\n","print(\"the cosine similarity between document 2 and 3 is:\",cosine_similarity(tf_idf[1],tf_idf[2]))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5fRYTpympAwJSVbric6dW","collapsed_sections":[],"name":"1a - word2vec.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"vscode":{"interpreter":{"hash":"89e7d167b8bde7d7cf9a172b3e3477586a36da8945a7654fbe033faf78f7d94a"}}},"nbformat":4,"nbformat_minor":0}
