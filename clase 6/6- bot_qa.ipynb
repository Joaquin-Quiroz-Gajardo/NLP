{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfa39F4lsLf3"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## LSTM Bot QA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqO0PRcFsPTe"
      },
      "source": [
        "### Datos\n",
        "El objecto es utilizar datos disponibles de convai de conversaciones en ingleś. Se construirá un BOT para responder a preguntas del usuario (QA).\\\n",
        "[LINK](http://convai.io/data/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bDFC0I3j9oFD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cq3YXak9sGHd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM, SimpleRNN\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RHNkUaPp6aYq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El dataset ya se encuentra descargado\n"
          ]
        }
      ],
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import os\n",
        "import gdown\n",
        "if os.access('data_volunteers.json', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1QPYxMTBSGNmbyGhknznuO3YWLBOJnhUQ&export=download'\n",
        "    output = 'data_volunteers.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WZy1-wgG-Rp7"
      },
      "outputs": [],
      "source": [
        "# dataset_file\n",
        "import json\n",
        "\n",
        "text_file = \"data_volunteers.json\"\n",
        "with open(text_file) as f:\n",
        "    data = json.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ue5qd54S-eew"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Observar los campos disponibles en cada linea del dataset\n",
        "data[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jHBRAXPl-3dz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de rows utilizadas: 6033\n"
          ]
        }
      ],
      "source": [
        "chat_in = []\n",
        "chat_out = []\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "max_len = 30\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()    \n",
        "    txt.replace(\"\\'d\", \" had\")\n",
        "    txt.replace(\"\\'s\", \" is\")\n",
        "    txt.replace(\"\\'m\", \" am\")\n",
        "    txt.replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "    \n",
        "    return txt\n",
        "\n",
        "for line in data:\n",
        "    for i in range(len(line['dialog'])-1):\n",
        "        chat_in = clean_text(line['dialog'][i]['text'])\n",
        "        chat_out = clean_text(line['dialog'][i+1]['text'])\n",
        "\n",
        "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
        "            continue\n",
        "\n",
        "        input_sentence, output = chat_in, chat_out\n",
        "        \n",
        "        # output sentence (decoder_output) tiene <eos>\n",
        "        output_sentence = output + ' <eos>'\n",
        "        # output sentence input (decoder_input) tiene <sos>\n",
        "        output_sentence_input = '<sos> ' + output\n",
        "\n",
        "        input_sentences.append(input_sentence)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "07L1qj8pC_l6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P-ynUNP5xp6"
      },
      "source": [
        "### 2 - Preprocesamiento\n",
        "Realizar el preprocesamiento necesario para obtener:\n",
        "- word2idx_inputs, max_input_len\n",
        "- word2idx_outputs, max_out_len, num_words_output\n",
        "- encoder_input_sequences, decoder_output_sequences, decoder_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir el tamaño máximo del vocabulario\n",
        "MAX_VOCAB_SIZE = 8000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras en el vocabulario: 1799\n",
            "Sentencia de entrada más larga: 9\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "input_tokenizer.fit_on_texts(input_sentences)\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
        "\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
        "\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"Sentencia de entrada más larga:\", max_input_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras en el vocabulario: 1806\n",
            "Sentencia de salida más larga: 10\n"
          ]
        }
      ],
      "source": [
        "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n",
        "output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
        "\n",
        "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) # Se suma 1 por el primer <sos>\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"Sentencia de salida más larga:\", max_out_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_input_len = 16\n",
        "max_out_len = 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de rows del dataset: 6033\n",
            "encoder_input_sequences shape: (6033, 16)\n",
            "decoder_input_sequences shape: (6033, 18)\n"
          ]
        }
      ],
      "source": [
        "print(\"Cantidad de rows del dataset:\", len(input_integer_seq))\n",
        "\n",
        "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
        "print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n",
        "\n",
        "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6033, 18, 1807)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "decoder_targets = to_categorical(decoder_output_sequences, num_classes=num_words_output)\n",
        "decoder_targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJIsLBbj6rg"
      },
      "source": [
        "### 3 - Preparar los embeddings\n",
        "Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Los embeddings gloveembedding.pkl ya están descargados\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gdown\n",
        "if os.access('gloveembedding.pkl', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download'\n",
        "    output = 'gloveembedding.pkl'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"Los embeddings gloveembedding.pkl ya están descargados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "import pickle\n",
        "\n",
        "class WordsEmbeddings(object):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    def __init__(self):\n",
        "        # load the embeddings\n",
        "        words_embedding_pkl = Path(self.PKL_PATH)\n",
        "        if not words_embedding_pkl.is_file():\n",
        "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
        "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
        "            embeddings = self.convert_model_to_pickle()\n",
        "        else:\n",
        "            embeddings = self.load_model_from_pickle()\n",
        "        self.embeddings = embeddings\n",
        "        # build the vocabulary hashmap\n",
        "        index = np.arange(self.embeddings.shape[0])\n",
        "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
        "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
        "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
        "\n",
        "    def get_words_embeddings(self, words):\n",
        "        words_idxs = self.words2idxs(words)\n",
        "        return self.embeddings[words_idxs]['embedding']\n",
        "\n",
        "    def words2idxs(self, words):\n",
        "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
        "\n",
        "    def load_model_from_pickle(self):\n",
        "        self.logger.debug(\n",
        "            'loading words embeddings from pickle {}'.format(\n",
        "                self.PKL_PATH\n",
        "            )\n",
        "        )\n",
        "        max_bytes = 2**28 - 1 # 256MB\n",
        "        bytes_in = bytearray(0)\n",
        "        input_size = os.path.getsize(self.PKL_PATH)\n",
        "        with open(self.PKL_PATH, 'rb') as f_in:\n",
        "            for _ in range(0, input_size, max_bytes):\n",
        "                bytes_in += f_in.read(max_bytes)\n",
        "        embeddings = pickle.loads(bytes_in)\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "    def convert_model_to_pickle(self):\n",
        "        # create a numpy strctured array:\n",
        "        # word     embedding\n",
        "        # U50      np.float32[]\n",
        "        # word_1   a, b, c\n",
        "        # word_2   d, e, f\n",
        "        # ...\n",
        "        # word_n   g, h, i\n",
        "        self.logger.debug(\n",
        "            'converting and loading words embeddings from text file {}'.format(\n",
        "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
        "            )\n",
        "        )\n",
        "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
        "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
        "        structure = np.dtype(structure)\n",
        "        # load numpy array from disk using a generator\n",
        "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
        "            embeddings_gen = (\n",
        "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
        "                if len(line.split()[1:]) == self.N_FEATURES\n",
        "            )\n",
        "            embeddings = np.fromiter(embeddings_gen, structure)\n",
        "        # add a null embedding\n",
        "        null_embedding = np.array(\n",
        "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
        "            dtype=structure\n",
        "        )\n",
        "        embeddings = np.concatenate([embeddings, null_embedding])\n",
        "        # dump numpy array to disk using pickle\n",
        "        max_bytes = 2**28 - 1 # # 256MB\n",
        "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.PKL_PATH, 'wb') as f_out:\n",
        "            for idx in range(0, len(bytes_out), max_bytes):\n",
        "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class GloveEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
        "    PKL_PATH = 'gloveembedding.pkl'\n",
        "    N_FEATURES = 50\n",
        "    WORD_MAX_SIZE = 60\n",
        "\n",
        "class FasttextEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
        "    PKL_PATH = 'fasttext.pkl'\n",
        "    N_FEATURES = 300\n",
        "    WORD_MAX_SIZE = 60\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_embeddings = GloveEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 38\n"
          ]
        }
      ],
      "source": [
        "print('preparing embedding matrix...')\n",
        "embed_dim = model_embeddings.N_FEATURES\n",
        "words_not_found = []\n",
        "\n",
        "# word_index provieen del tokenizer\n",
        "\n",
        "nb_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs)) # vocab_size\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        \n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        words_not_found.append(word)\n",
        "\n",
        "print('number of null word embeddings:', np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1800, 50)"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(type(embedding_matrix))\n",
        "embedding_matrix=np.r_[ embedding_matrix, [np.zeros(50)] ]\n",
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vKbhjtIwPgM"
      },
      "source": [
        "### 4 - Entrenar el modelo\n",
        "Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_input_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_39 (InputLayer)          [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " input_40 (InputLayer)          [(None, 18)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_35 (Embedding)       (None, 16, 50)       90000       ['input_39[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_36 (Embedding)       (None, 18, 128)      231296      ['input_40[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_26 (LSTM)                 [(None, 128),        91648       ['embedding_35[0][0]']           \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128)]                                                     \n",
            "                                                                                                  \n",
            " lstm_27 (LSTM)                 [(None, 18, 128),    131584      ['embedding_36[0][0]',           \n",
            "                                 (None, 128),                     'lstm_26[0][1]',                \n",
            "                                 (None, 128)]                     'lstm_26[0][2]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 18, 1807)     233103      ['lstm_27[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 777,631\n",
            "Trainable params: 687,631\n",
            "Non-trainable params: 90,000\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "\n",
        "n_units = 128\n",
        "\n",
        "# define training encoder\n",
        "encoder_inputs = Input(shape=(max_input_len))\n",
        "\n",
        "#encoder_embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)\n",
        "\n",
        "encoder_embedding_layer = Embedding(\n",
        "          input_dim=nb_words+1,  # definido en el Tokenizador\n",
        "          output_dim=embed_dim,  # dimensión de los embeddings utilizados\n",
        "          input_length=max_input_len, # máxima sentencia de entrada\n",
        "          weights=[embedding_matrix],  # matrix de embeddings\n",
        "          trainable=False)      # marcar como layer no entrenable\n",
        "\n",
        "encoder_inputs_x = encoder_embedding_layer(encoder_inputs)\n",
        "\n",
        "encoder = LSTM(n_units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs_x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# define training decoder\n",
        "decoder_inputs = Input(shape=(max_out_len))\n",
        "decoder_embedding_layer = Embedding(input_dim=num_words_output, output_dim=n_units, input_length=max_out_len)\n",
        "decoder_inputs_x = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
        "\n",
        "# Dense\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(encoder_model, to_file='encoder_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "# Modelo solo decoder (para realizar inferencia)\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "# define inference decoder\n",
        "decoder_state_input_h = Input(shape=(n_units,))\n",
        "decoder_state_input_c = Input(shape=(n_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# EN cada predicción habrá una sola palabra de entrada al decoder,\n",
        "# que es la realimentación de la palabra anterior\n",
        "# por lo que hay que modificar el input shup de la layer de Embedding\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding_layer(decoder_inputs_single)\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs_single] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "plot_model(decoder_model, to_file='decoder_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "151/151 [==============================] - 14s 60ms/step - loss: 2.1275 - accuracy: 0.7252 - val_loss: 1.2963 - val_accuracy: 0.7846\n",
            "Epoch 2/10\n",
            "151/151 [==============================] - 7s 49ms/step - loss: 1.1794 - accuracy: 0.7881 - val_loss: 1.1860 - val_accuracy: 0.8019\n",
            "Epoch 3/10\n",
            "151/151 [==============================] - 7s 48ms/step - loss: 1.0638 - accuracy: 0.8087 - val_loss: 1.1046 - val_accuracy: 0.8194\n",
            "Epoch 4/10\n",
            "151/151 [==============================] - 7s 48ms/step - loss: 0.9628 - accuracy: 0.8327 - val_loss: 1.0477 - val_accuracy: 0.8295\n",
            "Epoch 5/10\n",
            "151/151 [==============================] - 7s 48ms/step - loss: 0.8927 - accuracy: 0.8418 - val_loss: 1.0102 - val_accuracy: 0.8326\n",
            "Epoch 6/10\n",
            "151/151 [==============================] - 7s 47ms/step - loss: 0.8406 - accuracy: 0.8482 - val_loss: 0.9812 - val_accuracy: 0.8385\n",
            "Epoch 7/10\n",
            "151/151 [==============================] - 7s 49ms/step - loss: 0.8016 - accuracy: 0.8532 - val_loss: 0.9613 - val_accuracy: 0.8395\n",
            "Epoch 8/10\n",
            "151/151 [==============================] - 7s 47ms/step - loss: 0.7701 - accuracy: 0.8567 - val_loss: 0.9475 - val_accuracy: 0.8419\n",
            "Epoch 9/10\n",
            "151/151 [==============================] - 7s 47ms/step - loss: 0.7439 - accuracy: 0.8593 - val_loss: 0.9348 - val_accuracy: 0.8433\n",
            "Epoch 10/10\n",
            "151/151 [==============================] - 7s 48ms/step - loss: 0.7214 - accuracy: 0.8610 - val_loss: 0.9266 - val_accuracy: 0.8439\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit(\n",
        "    [encoder_input_sequences, decoder_input_sequences],\n",
        "    decoder_targets,\n",
        "    epochs=10, \n",
        "    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=hist.history['val_accuracy'], label='valid')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbwn0ekDy_s2"
      },
      "source": [
        "### 5 - Inferencia\n",
        "Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_sentence(input_seq):\n",
        "    # Se transforma la sequencia de entrada a los stados \"h\" y \"c\" de la LSTM\n",
        "    # para enviar la primera vez al decoder\"\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "    # Se obtiene el indice que finaliza la inferencia\n",
        "    eos = word2idx_outputs['<eos>']\n",
        "    \n",
        "    output_sentence = []\n",
        "    for _ in range(max_out_len):\n",
        "        # Predicción del próximo elemento\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "        # Si es \"end of sentece <eos>\" se acaba\n",
        "        if eos == idx:\n",
        "            break\n",
        "\n",
        "        # Transformar ídx a palabra\n",
        "        word = ''        \n",
        "        if idx > 0:\n",
        "            word = idx2word_target[idx]\n",
        "            output_sentence.append(word)\n",
        "\n",
        "        # Actualizar los estados dado la ultimo prediccion\n",
        "        states_value = [h, c]\n",
        "\n",
        "        # Actualizar secuencia de entrada con la salida (re-alimentacion)\n",
        "        target_seq[0, 0] = idx\n",
        "\n",
        "    return ' '.join(output_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: Hello\n",
            "Representacion en vector de tokens de ids [11, 10, 7, 2]\n",
            "Padding del vector: [[ 0  0  0  0  0  0  0  0  0  0  0  0 11 10  7  2]]\n",
            "Input: Hello\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "Response: Hi"
          ]
        }
      ],
      "source": [
        "input_test = \"Hello\"\n",
        "print('Input:', input_test)\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "print(\"Representacion en vector de tokens de ids\", integer_seq_test)\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "print(\"Padding del vector:\", encoder_sequence_test)\n",
        "\n",
        "print('Input:', input_test)\n",
        "translation = translate_sentence(encoder_sequence_test)\n",
        "print('Response:', translation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "6d - bot_qa.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e7d167b8bde7d7cf9a172b3e3477586a36da8945a7654fbe033faf78f7d94a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
