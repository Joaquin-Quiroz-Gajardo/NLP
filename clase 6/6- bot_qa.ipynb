{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfa39F4lsLf3"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## LSTM Bot QA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqO0PRcFsPTe"
      },
      "source": [
        "### Datos\n",
        "El objecto es utilizar datos disponibles de convai de conversaciones en ingleś. Se construirá un BOT para responder a preguntas del usuario (QA).\\\n",
        "[LINK](http://convai.io/data/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bDFC0I3j9oFD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cq3YXak9sGHd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM, SimpleRNN\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RHNkUaPp6aYq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QPYxMTBSGNmbyGhknznuO3YWLBOJnhUQ&export=download\n",
            "To: c:\\Users\\joaqu\\OneDrive\\Documentos\\drive\\uba\\nlp\\desafios\\clase 6\\data_volunteers.json\n",
            "100%|██████████| 2.58M/2.58M [00:00<00:00, 2.84MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import os\n",
        "import gdown\n",
        "if os.access('data_volunteers.json', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1QPYxMTBSGNmbyGhknznuO3YWLBOJnhUQ&export=download'\n",
        "    output = 'data_volunteers.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WZy1-wgG-Rp7"
      },
      "outputs": [],
      "source": [
        "# dataset_file\n",
        "import json\n",
        "\n",
        "text_file = \"data_volunteers.json\"\n",
        "with open(text_file) as f:\n",
        "    data = json.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ue5qd54S-eew"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Observar los campos disponibles en cada linea del dataset\n",
        "data[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jHBRAXPl-3dz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de rows utilizadas: 6033\n"
          ]
        }
      ],
      "source": [
        "chat_in = []\n",
        "chat_out = []\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "max_len = 30\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()    \n",
        "    txt.replace(\"\\'d\", \" had\")\n",
        "    txt.replace(\"\\'s\", \" is\")\n",
        "    txt.replace(\"\\'m\", \" am\")\n",
        "    txt.replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "    \n",
        "    return txt\n",
        "\n",
        "for line in data:\n",
        "    for i in range(len(line['dialog'])-1):\n",
        "        chat_in = clean_text(line['dialog'][i]['text'])\n",
        "        chat_out = clean_text(line['dialog'][i+1]['text'])\n",
        "\n",
        "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
        "            continue\n",
        "\n",
        "        input_sentence, output = chat_in, chat_out\n",
        "        \n",
        "        # output sentence (decoder_output) tiene <eos>\n",
        "        output_sentence = output + ' <eos>'\n",
        "        # output sentence input (decoder_input) tiene <sos>\n",
        "        output_sentence_input = '<sos> ' + output\n",
        "\n",
        "        input_sentences.append(input_sentence)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "07L1qj8pC_l6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P-ynUNP5xp6"
      },
      "source": [
        "### 2 - Preprocesamiento\n",
        "Realizar el preprocesamiento necesario para obtener:\n",
        "- word2idx_inputs, max_input_len\n",
        "- word2idx_outputs, max_out_len, num_words_output\n",
        "- encoder_input_sequences, decoder_output_sequences, decoder_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir el tamaño máximo del vocabulario\n",
        "MAX_VOCAB_SIZE = 8000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras en el vocabulario: 1799\n",
            "Sentencia de entrada más larga: 9\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "input_tokenizer.fit_on_texts(input_sentences)\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
        "\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
        "\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"Sentencia de entrada más larga:\", max_input_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras en el vocabulario: 1806\n",
            "Sentencia de salida más larga: 10\n"
          ]
        }
      ],
      "source": [
        "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n",
        "output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
        "\n",
        "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) # Se suma 1 por el primer <sos>\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"Sentencia de salida más larga:\", max_out_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_input_len = 16\n",
        "max_out_len = 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de rows del dataset: 6033\n",
            "encoder_input_sequences shape: (6033, 16)\n",
            "decoder_input_sequences shape: (6033, 18)\n"
          ]
        }
      ],
      "source": [
        "print(\"Cantidad de rows del dataset:\", len(input_integer_seq))\n",
        "\n",
        "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
        "print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n",
        "\n",
        "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6033, 18, 1807)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "decoder_targets = to_categorical(decoder_output_sequences, num_classes=num_words_output)\n",
        "decoder_targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJIsLBbj6rg"
      },
      "source": [
        "### 3 - Preparar los embeddings\n",
        "Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download\n",
            "To: c:\\Users\\joaqu\\OneDrive\\Documentos\\drive\\uba\\nlp\\desafios\\clase 6\\gloveembedding.pkl\n",
            "100%|██████████| 525M/525M [02:15<00:00, 3.86MB/s] \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gdown\n",
        "if os.access('gloveembedding.pkl', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download'\n",
        "    output = 'gloveembedding.pkl'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"Los embeddings gloveembedding.pkl ya están descargados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "import pickle\n",
        "\n",
        "class WordsEmbeddings(object):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    def __init__(self):\n",
        "        # load the embeddings\n",
        "        words_embedding_pkl = Path(self.PKL_PATH)\n",
        "        if not words_embedding_pkl.is_file():\n",
        "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
        "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
        "            embeddings = self.convert_model_to_pickle()\n",
        "        else:\n",
        "            embeddings = self.load_model_from_pickle()\n",
        "        self.embeddings = embeddings\n",
        "        # build the vocabulary hashmap\n",
        "        index = np.arange(self.embeddings.shape[0])\n",
        "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
        "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
        "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
        "\n",
        "    def get_words_embeddings(self, words):\n",
        "        words_idxs = self.words2idxs(words)\n",
        "        return self.embeddings[words_idxs]['embedding']\n",
        "\n",
        "    def words2idxs(self, words):\n",
        "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
        "\n",
        "    def load_model_from_pickle(self):\n",
        "        self.logger.debug(\n",
        "            'loading words embeddings from pickle {}'.format(\n",
        "                self.PKL_PATH\n",
        "            )\n",
        "        )\n",
        "        max_bytes = 2**28 - 1 # 256MB\n",
        "        bytes_in = bytearray(0)\n",
        "        input_size = os.path.getsize(self.PKL_PATH)\n",
        "        with open(self.PKL_PATH, 'rb') as f_in:\n",
        "            for _ in range(0, input_size, max_bytes):\n",
        "                bytes_in += f_in.read(max_bytes)\n",
        "        embeddings = pickle.loads(bytes_in)\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "    def convert_model_to_pickle(self):\n",
        "        # create a numpy strctured array:\n",
        "        # word     embedding\n",
        "        # U50      np.float32[]\n",
        "        # word_1   a, b, c\n",
        "        # word_2   d, e, f\n",
        "        # ...\n",
        "        # word_n   g, h, i\n",
        "        self.logger.debug(\n",
        "            'converting and loading words embeddings from text file {}'.format(\n",
        "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
        "            )\n",
        "        )\n",
        "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
        "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
        "        structure = np.dtype(structure)\n",
        "        # load numpy array from disk using a generator\n",
        "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
        "            embeddings_gen = (\n",
        "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
        "                if len(line.split()[1:]) == self.N_FEATURES\n",
        "            )\n",
        "            embeddings = np.fromiter(embeddings_gen, structure)\n",
        "        # add a null embedding\n",
        "        null_embedding = np.array(\n",
        "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
        "            dtype=structure\n",
        "        )\n",
        "        embeddings = np.concatenate([embeddings, null_embedding])\n",
        "        # dump numpy array to disk using pickle\n",
        "        max_bytes = 2**28 - 1 # # 256MB\n",
        "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.PKL_PATH, 'wb') as f_out:\n",
        "            for idx in range(0, len(bytes_out), max_bytes):\n",
        "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class GloveEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
        "    PKL_PATH = 'gloveembedding.pkl'\n",
        "    N_FEATURES = 50\n",
        "    WORD_MAX_SIZE = 60\n",
        "\n",
        "class FasttextEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
        "    PKL_PATH = 'fasttext.pkl'\n",
        "    N_FEATURES = 300\n",
        "    WORD_MAX_SIZE = 60\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_embeddings = GloveEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 38\n"
          ]
        }
      ],
      "source": [
        "print('preparing embedding matrix...')\n",
        "embed_dim = model_embeddings.N_FEATURES\n",
        "words_not_found = []\n",
        "\n",
        "# word_index provieen del tokenizer\n",
        "\n",
        "nb_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs)) # vocab_size\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        \n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        words_not_found.append(word)\n",
        "\n",
        "print('number of null word embeddings:', np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1800, 50)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(type(embedding_matrix))\n",
        "embedding_matrix=np.r_[ embedding_matrix, [np.zeros(50)] ]\n",
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vKbhjtIwPgM"
      },
      "source": [
        "### 4 - Entrenar el modelo\n",
        "Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_input_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 18)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 16, 50)       90000       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 18, 128)      231296      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 128),        91648       ['embedding[0][0]']              \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 18, 128),    131584      ['embedding_1[0][0]',            \n",
            "                                 (None, 128),                     'lstm[0][1]',                   \n",
            "                                 (None, 128)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 18, 1807)     233103      ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 777,631\n",
            "Trainable params: 687,631\n",
            "Non-trainable params: 90,000\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "\n",
        "n_units = 128\n",
        "\n",
        "# define training encoder\n",
        "encoder_inputs = Input(shape=(max_input_len))\n",
        "\n",
        "#encoder_embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)\n",
        "\n",
        "encoder_embedding_layer = Embedding(\n",
        "          input_dim=nb_words+1,  # definido en el Tokenizador\n",
        "          output_dim=embed_dim,  # dimensión de los embeddings utilizados\n",
        "          input_length=max_input_len, # máxima sentencia de entrada\n",
        "          weights=[embedding_matrix],  # matrix de embeddings\n",
        "          trainable=False)      # marcar como layer no entrenable\n",
        "\n",
        "encoder_inputs_x = encoder_embedding_layer(encoder_inputs)\n",
        "\n",
        "encoder = LSTM(n_units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs_x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# define training decoder\n",
        "decoder_inputs = Input(shape=(max_out_len))\n",
        "decoder_embedding_layer = Embedding(input_dim=num_words_output, output_dim=n_units, input_length=max_out_len)\n",
        "decoder_inputs_x = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
        "\n",
        "# Dense\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(encoder_model, to_file='encoder_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "# Modelo solo decoder (para realizar inferencia)\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "# define inference decoder\n",
        "decoder_state_input_h = Input(shape=(n_units,))\n",
        "decoder_state_input_c = Input(shape=(n_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# EN cada predicción habrá una sola palabra de entrada al decoder,\n",
        "# que es la realimentación de la palabra anterior\n",
        "# por lo que hay que modificar el input shup de la layer de Embedding\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding_layer(decoder_inputs_single)\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs_single] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "plot_model(decoder_model, to_file='decoder_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "151/151 [==============================] - 53s 193ms/step - loss: 2.0833 - accuracy: 0.7253 - val_loss: 1.3018 - val_accuracy: 0.7787\n",
            "Epoch 2/10\n",
            "151/151 [==============================] - 12s 81ms/step - loss: 1.1801 - accuracy: 0.7904 - val_loss: 1.1836 - val_accuracy: 0.8036\n",
            "Epoch 3/10\n",
            "151/151 [==============================] - 12s 76ms/step - loss: 1.0634 - accuracy: 0.8106 - val_loss: 1.1045 - val_accuracy: 0.8196\n",
            "Epoch 4/10\n",
            "151/151 [==============================] - 11s 75ms/step - loss: 0.9610 - accuracy: 0.8326 - val_loss: 1.0483 - val_accuracy: 0.8276\n",
            "Epoch 5/10\n",
            "151/151 [==============================] - 11s 75ms/step - loss: 0.8892 - accuracy: 0.8415 - val_loss: 1.0066 - val_accuracy: 0.8334\n",
            "Epoch 6/10\n",
            "151/151 [==============================] - 11s 74ms/step - loss: 0.8381 - accuracy: 0.8469 - val_loss: 0.9812 - val_accuracy: 0.8371\n",
            "Epoch 7/10\n",
            "151/151 [==============================] - 11s 75ms/step - loss: 0.8001 - accuracy: 0.8516 - val_loss: 0.9623 - val_accuracy: 0.8389\n",
            "Epoch 8/10\n",
            "151/151 [==============================] - 11s 75ms/step - loss: 0.7715 - accuracy: 0.8554 - val_loss: 0.9470 - val_accuracy: 0.8405\n",
            "Epoch 9/10\n",
            "151/151 [==============================] - 11s 72ms/step - loss: 0.7473 - accuracy: 0.8575 - val_loss: 0.9356 - val_accuracy: 0.8427\n",
            "Epoch 10/10\n",
            "151/151 [==============================] - 11s 73ms/step - loss: 0.7258 - accuracy: 0.8591 - val_loss: 0.9247 - val_accuracy: 0.8446\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit(\n",
        "    [encoder_input_sequences, decoder_input_sequences],\n",
        "    decoder_targets,\n",
        "    epochs=10, \n",
        "    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV5b3v8c8vc0ImMgIJmDCPATEMilO1VhwAh9aCQytVqdY6tadVe+85nnvbnuu99dxTe2vlUIu0iijFAVRaHE5VqhUJGiDMEZAMQCYSMpJh/+4fawc2ISGbZCc72fm9Xy9eyVrrWWv/9n7J14dnP+tZoqoYY4wJXEH+LsAYY0zPsqA3xpgAZ0FvjDEBzoLeGGMCnAW9McYEuBB/F9CepKQkzcjI8HcZxhjTb2zZsqVMVZPbO9Yngz4jI4OcnBx/l2GMMf2GiHzV0TEbujHGmABnQW+MMQHOgt4YYwJcnxyjb09TUxOFhYU0NDT4u5QeFxERQXp6OqGhof4uxRgTALwKehGZCzwNBAPPqeqTbY7HAS8CI9zXfEpVn3cfiweeAyYDCnxPVf9xroUWFhYSExNDRkYGInKup/cbqkp5eTmFhYVkZmb6uxxjTADodOhGRIKBZ4BrgInAIhGZ2KbZ/cBOVZ0KXA78u4iEuY89DfxVVccDU4FdXSm0oaGBxMTEgA55ABEhMTFxQPzLxRjTO7wZo58J5KvqflVtBF4GFrRpo0CMOCkcDVQAzSISC1wK/AFAVRtVtbKrxQZ6yLcaKO/TGNM7vBm6SQMKPLYLgVlt2vwWWAcUAzHAt1XVJSIjgVLgeRGZCmwBHlLV2rYvIiJLgCUAI0aMONf3YYwxfZ6qUtfYQmV9E5V1jVTVN1FV1+TebkIE7r1slM9f15ugb6972XYR+6uBXOAKYBTwrohsdF9/OvCAqm4SkaeBx4B/PuOCqsuAZQDZ2dl9bpH8yspKXnrpJX7wgx+c03nXXnstL730EvHx8T1UmTGmt7W4lOP1TScDu9Id2FXuwK6sbzy17Rnq9U00tXQcb8kx4X4L+kJguMd2Ok7P3dNi4El1nmKSLyIHgPHAIaBQVTe5263BCfp+p7Kykt/97ndnBH1LSwvBwcEdnrd+/fqeLs0Y00UtLqWitpGK2sbTAruyvtEjtD1CvL6Ryromqhuaz3rdmPAQYiNDiY9y/owfEntq2/0zLjKUuMiwk23iI8OICO2ZGe/eBP1mYIyIZAJFwELg1jZtDgFXAhtFJBUYB+xX1TIRKRCRcaq6x91mp+/K7z2PPfYYX375JdOmTSM0NJTo6GiGDh1Kbm4uO3fu5IYbbqCgoICGhgYeeughlixZApxazqGmpoZrrrmGiy++mE8++YS0tDTWrl1LZGSkn9+ZMYHF5VKO1TVSVtNIafUJympOnPaztObEyWMVtSdwddDBDg4S4iNDiXOHclJ0GKNTot0BHXpaQHuGeGxkKKHBfesWpU6DXlWbReSHwAac6ZXLVXWHiNzrPr4U+DmwQkS24wz1PKqqZe5LPACsdM/C2Y/T+++W//HmDnYWH+/uZU4zcVgsT8yb1OHxJ598kry8PHJzc/nggw+47rrryMvLOzkFcvny5SQkJFBfX8+MGTO4+eabSUxMPO0a+/btY9WqVfz+97/nlltu4dVXX+X222/36fswJhC5XEpVfZMT0u6wdsL7zDAvr22kpZ30DgsJIjk6nKToMNLiI5iaHkdyTDhJ0eEkRocxOCrsZIDHRYYSHR4SMBMjvJpHr6rrgfVt9i31+L0Y+EYH5+YC2d2osU+aOXPmafPcf/Ob3/D6668DUFBQwL59+84I+szMTKZNmwbABRdcwMGDB3utXmP6oqr6JkqrGyitbjwtxE/+rDlBWXUjZTUnaG4nvEODhaTocJJjwhkSF8HktNiT4d32Z2xE4AT3ueo3d8Z6OlvPu7cMGjTo5O8ffPAB7733Hv/4xz+Iiori8ssvb3cefHh4+Mnfg4ODqa+v75VajfEnVaW8tpF9R2vIL6lmX0kNe49Wk19SQ1lN4xntQ4KExOiwkwE9fkjb8A4jxb0dFxk6YMP7XPTLoPeHmJgYqqur2z1WVVXF4MGDiYqKYvfu3Xz66ae9XJ0x/qeqlFafYO/RGva5Az3f/fuxuqaT7aLDQxidEs3XxqUwOiWaIXERp/W84yNDCQqy8PYlC3ovJSYmMmfOHCZPnkxkZCSpqaknj82dO5elS5eSlZXFuHHjmD17th8rNaZnqSqHqxrYV1LDPnfPvPX34x6zUWIjQhibGsPcyUMYnRLDmJRoxqRGMyQ2wnrhvUycGZF9S3Z2trZ98MiuXbuYMGGCnyrqfQPt/Zq+x+VSiirr3UFezb6jTqDnl9RQc+JUoA+OCmVMqjvIU6IZmxrD6NRokqPDLdB7kYhsUdV2vw+1Hr0xA1yLSyk8Vse+ozXsLal2D7c4gV7f1HKyXVJ0OGNTo7l5ehqjPYI9MTr8LFc3fYEFvTEDyLHaRrYXVbG9qIq9R51e+pelNZxodp1sMyQ2gjGp0SycOZwxKTGMSY1mdHI0gweFneXKpi+zoDcmQFU3NJFXdJxthZVsK6pie2EVhyrqTh5Pi49kdEo0F41KPDncMjolmtgIew5CoLGgNyYA1DU2s7P4ONsKq04G+/7SU2sHpg+OJCs9jkUzR5CVHsfktDjiIi3QBwoLemP6mRPNLew6XM32wkq2FZ4ahmm9nyg1NpwpafHcOC2NKelxTEmLs3H0Ac6C3pg+rKnFxd6j1WwvrGJrYRXbiyrZc6T65AqICYPCyEqP4xsTU8lKj2dKehypsRF+rtr0NRb0PSQ6OpqamhqKi4t58MEHWbNmzRltLr/8cp566imyswNuhQjTBS0u5cvSGqeX7h5+2Vl8/OQXpbERIWSlx3P3JSPJSotjSnocafGRNoXRdMqCvocNGzas3ZA3A5vLpXxVUeeMpxc6X5TmFVdR1+hMZ4wKC2ZyWhx3zD6PrOHxZKXFcV5ilIW66RILei89+uijnHfeeSfXo//Xf/1XRISPPvqIY8eO0dTUxC9+8QsWLDj9KYsHDx7k+uuvJy8vj/r6ehYvXszOnTuZMGGCrXUzgBxvaGLLV8fYfKCCre5wb13TPDwkiEnDYrklezhT0uLISo9jZHI0wbYMgPGR/hn0f3kMjmz37TWHTIFrnuzw8MKFC3n44YdPBv3q1av561//yiOPPEJsbCxlZWXMnj2b+fPnd9jrevbZZ4mKimLbtm1s27aN6dOn+/Y9mD6jpLqBzQeOsflgBZ8dqGD3keO41Fmwa/zQGOZNHcbU9DimpMUzJjW6z61fbgJL/wx6Pzj//PMpKSmhuLiY0tJSBg8ezNChQ3nkkUf46KOPCAoKoqioiKNHjzJkyJB2r/HRRx/x4IMPApCVlUVWVlZvvgXTQ1SVQxV1fHaggs0HK9h88BgHypypjZGhwZw/Ip4HrhjDrMwEpo2IJyrM/tqZ3tU//4s7S8+7J33zm99kzZo1HDlyhIULF7Jy5UpKS0vZsmULoaGhZGRktLs8sScbY+3/XC5lz9Hqk731zQcrOHr8BABxkaHMyBjMopnDmZGRwOS0OOutG+80NcCJ4xCd4vNL98+g95OFCxdyzz33UFZWxocffsjq1atJSUkhNDSUv/3tb3z11VdnPf/SSy9l5cqVfO1rXyMvL49t27b1UuWmOxqbXWwvqnJ66+5gb12lcWhcBLMyE5mRmcDMjATGpETbErsGVKGhEmrLoa4c6sqcn7Xun6f9Xua0a6qFmKHw490+L8eC/hxMmjSJ6upq0tLSGDp0KLfddhvz5s0jOzubadOmMX78+LOef99997F48WKysrKYNm0aM2fO7KXKzbmoPdHMF4cq+exAOZ8drCC3oJKGJmeK48jkQVw7ZSgzMhKYmZlA+mCb3jggNDeeCui6MndIV5w9wLWl/WuFRkFUEgxKhKhESBrr/ByUCNHtD/t2ly1T3EcNtPfrTxW1jaf11vOKj9PiUoLEeZbwjIwEZmUmkJ2RQJLdYRoYXC4njGuOQPVRqDl69gA/0dEzqgUiB8OgJCesoxI9fk86FeBRHsfDonrkLdkyxcZ4KKqsZ/OBCj5zh/u+khrAeXj0tOHx3HfZKGZkJjB9RDwxtsBX/9LSDLWlHgF+BKrdf2qOnvq9tgRczWeeHxx+emgPzjgV2q098JMBnuSEfFBwr7/Nc+VV0IvIXOBpIBh4TlWfbHM8DngRGOG+5lOq+rzH8WAgByhS1et9VLsxXmloamHDjiN8sKeUzw5UUFTp3L8QEx7CBRmDueH8NGZmJpCVHkd4SN//SzsgNTc64XwytN1BXn34VIDXHHVCXl1nnh+VBDFDIDoVUia4fx8CManOz+gUGJQMYYMgAIfiOg16d0g/A1wFFAKbRWSdqu70aHY/sFNV54lIMrBHRFaqauuTfx8CdgGx3SlWVQfEeGhfHE7rj/KKqlidU8AbXxRxvKGZpOhwZmYO5u5LMpmZmcD4IbF2U5K/NdW7g7qd0K4+fKpXXld+5rkS5IRzdKrzJeawaafCO2boqd8HpUDIwF5L35se/UwgX1X3A4jIy8ACwDPoFYgRJ4WjgQqg2d0+HbgO+CXwo64WGhERQXl5OYmJiQEd9qpKeXk5ERG2MFVXVNU18UZuEa9sLmDn4eOEhQQxd9IQvj1jOBeOTLQZMT3B1QINVVB/zJlpUn8M6is9tivbbHscb27n7vCgECe8o1Nh8HkwfKYT3K2979Ygj0qCYBt99oY3n1IaUOCxXQjMatPmt8A6oBiIAb6tevLfT78Gfure3yERWQIsARgxYsQZx9PT0yksLKS0tNSLkvu3iIgI0tPT/V1Gv+FyKZ98Wc7qnAL+uuMIjc0uJg2L5X8umMSCqWnERdk4e6dUobG2g6DuZLuh6uzXDo1yxrIj4p2fCSMhMv7UdmuPvDXIoxIhyO498CVvgr69LlDbsYWrgVzgCmAU8K6IbAQuBUpUdYuIXH62F1HVZcAycGbdtD0eGhpKZmamF+WagaKosp41OYX8eUsBhcfqiY0IYdGM4XwreziT0+L8XZ7/qDqzRNqdQeLeV1t2ZnC7mjq+ZlDIqWCOjHfGtJPGntr2DPLTtuMhxGYq+Zs3QV8IDPfYTsfpuXtaDDypzuByvogcAMYDc4D5InItEAHEisiLqnp790s3A9GJ5hbe21nCKzkFbNxXiirMGZ3IT64ex9WThhARGoBfprY0dT5n2zPA68o7Du2Ts0oSIDIBYod1EtTu7bDogPyScqDwJug3A2NEJBMoAhYCt7Zpcwi4EtgoIqnAOGC/qj4OPA7g7tH/k4W86YrdR47zymbni9VjdU0Mi4vggSvG8K0L0hme0DPzkntE6xBJ692QZ9w12Saw68rOPjQSEX9qql/8CBh2/tnncQforBJzdp0Gvao2i8gPgQ040yuXq+oOEbnXfXwp8HNghYhsxxnqeVRVy3qwbjMAHG9oYl1uMX/OKWBrYRVhwUFcNSmVW7KHc/HopL49Y8bVAscOQtleKN3j/rkbyvZ1fPNNUKjHDTcJMHTq6XO6Tx5r3ZcAwfb9g+lcv7kz1gwMqsqmAxWs3lzA+rzDNDS5GD8khluyh3PD+WkkDOpj0+SaT0B5vkeY73H+lOdDy4lT7aJTnTHt5HEQN7xNrzvB2Q6Ptd626TK7M9b0eUeqGnj180JW5xTwVXkdMeEh3DQ9nW9nDycrPc7/U2pPVEPpXijbc3qoHzvgcYOOOMMnyeNg9BWQNM75PWmsM85tjJ9Y0Bu/aWpx8f6uElbnFPDBnhJcCrMyE3joyjFcM3kokWF++GK1tswZYvEM87K9cLzoVJugUEgcBamTYPLNp8I8cXSPrWNiTHdY0Jtel19SzSubC3j9iyLKahpJiQnn3stGcUv2cDKSBvV8AapQVegO8TY99PqKU+1CB0HSGMi42D3sMt4J9cEZNjZu+hULetMrak408/a2Yl7ZXMDnhyoJCRKunJDCt2cM59IxyYT01MM5XC4o2QkHN0LxF+5Q3+es/d0qMsEJ8AnznJ/J45xhl9g0u3HHBAQLetOjCirq+M37+3h7+2HqGlsYnRLNf7t2Ajecn0ZyTA/cSKPqDL0c2AgHP4KDH5/qpccMc0J8+h2nvhhNHu98EWpMALOgNz0mv6SaW3+/iZoTzczLGsYtM4YzfUS8b79YVXVmuBz4yOm1H/y7s4IhQNwIGHcNZFziDL/EDz/7tYwJUBb0pkfsOnyc25/bhIjwxv1zGJt61qWOvKcKFfudUD/gDvaaI86xmGEw6gon2DMvccbSjTEW9Mb3thdWccfyTUSEBPPSPbMYmRzdvQse+8oj2DeemgETnXoq1DMucRbL8vc0TGP6IAt641NbvqrgzuWbiYsK5aW7ZzMisQvTDasKT4X6wY1QecjZH5XkDMFk/ggyLnVmxFiwG9MpC3rjM//4spy7/riZ1NgIVt49i2Hxkd6dePywMwRz8CMn4I8dcPZHDnaC/cIHnJ8pEyzYjekCC3rjEx/tLeWeP+UwIiGKlXfPIiX2LA9OqSk5fSimPN/ZHx4HGXNg5hJnOCZlkk1vNMYHLOhNt7238yg/WPk5o1KiefGumSRGt5k2WVsOX/39VLCX7nb2h8XAeRfB9O86wT4kq188aNmY/saC3nTL+u2HeXDVF0waFssfvzeT+Cj3omM1JbDjDchbAwWbnH2hg2DEbJi60BljHzrVHgVnTC+wv2Wmy17/opAfr97K9BGDWb54BrHUwRerYfsaOPChs9hXykS4/Gcw8nJIm25LBxjjBxb0pkte/uwQj7++nUsyolk26zARb9wJ+96BlkaIPw8ufgQmfxNSJ/q7VGMGPAt6c85e+Hgf77/9Ci8O/pyLyjchb9Q4c9qz74Ip34S0C2x2jDF9iAW98Y7LBYc+Yec7y7muaAN3hNWgrjhk8k1Ozz3jYvsi1Zg+yoLedEzVWfEx71XIew2qi8nQcHbGziH2mrsIGXsVhPTAwmTGGJ+yoDdnKt3rzJbZvgYqvkSDQvkybja/abyJqKzr+cW3ZvXcssLGGJ+zoDeOygJ3z30NHHE/4z3zEnTOQ/zq0Fh+t6mCRTNH8MsbJhPUlx/KbYw5gwX9QFZTCjvfcHruBZ86+9KyYe6TMOlGXINS+ee1eazcdIg7L8rgiXkT/f/sVmPMOfMq6EVkLvA0EAw8p6pPtjkeB7wIjHBf8ylVfV5EhgN/AoYALmCZqj7tw/rNuWqogl1vOT33/R+Ctjhz3a/4Z+f5pwmZALS4lEdf3caaLYXce9koHp07zkLemH6q06AXkWDgGeAqoBDYLCLrVHWnR7P7gZ2qOk9EkoE9IrISaAZ+rKqfi0gMsEVE3m1zrulpTfWwd4MT7nvfgZYTzlz3OQ850yFTJ53evMXFj1dvZd3WYh7++hgeunKMhbwx/Zg3PfqZQL6q7gcQkZeBBYBnWCsQI04aRAMVQLOqHgYOA6hqtYjsAtLanGt6gssFX/4XbP8z7H4bGqvdc90XO9Mh07Pbneve2OzigVWfs2HHUR6dO577Lh/lh+KNMb7kTdCnAQUe24XArDZtfgusA4qBGODbqurybCAiGcD5wKb2XkRElgBLAEaMGOFFWaZDxw/DG/fB/r9BRBxMusHpuWdccta57g1NLdz34hb+tqeUJ+ZNZPGczF4s2hjTU7wJ+vb+za5ttq8GcoErgFHAuyKyUVWPA4hINPAq8HDrvjMuqLoMWAaQnZ3d9vrGWzvegLcehuYTcN2/w/l3eDXXva6xmXv+lMMnX5bzbzdO4dZZ9j9bYwKFN0FfCHg+VTkdp+fuaTHwpKoqkC8iB4DxwGciEooT8itV9TUf1Gza01AFf3kUtq5yliC4cRkkjfbq1OqGJu5akUPOVxU89c2p3HxBeg8Xa4zpTd4E/WZgjIhkAkXAQuDWNm0OAVcCG0UkFRgH7HeP2f8B2KWq/9d3ZZvTfPUJvPZ951mqlz0Gl/6T16tEVtU38d3ln7G9qIqnF57PvKnDerhYY0xv6zToVbVZRH4IbMCZXrlcVXeIyL3u40uBnwMrRMR9pw2PqmqZiFwM3AFsF5Fc9yV/pqrre+LNDDjNjfDBv8Hffw2DM+B7G2D4DK9Pr6ht5I4/bGLv0Wp+d9t0rp40pOdqNcb4jVfz6N3BvL7NvqUevxcD32jnvL/T/hi/6a6S3fDaPXBkm/OEpqv/DcKjvT+9uoE7nvuMg+W1/P472Vw+LqUHizXG+JPdGdvfuFzw2TJ47wkIi4aFq2D8ted0icNV9dz2+00crmrg+TtncNHopB4q1hjTF1jQ9yfHi+GNHzjTJsfOhfn/D6LPrSdeUFHHrc99yrHaJl64aybZGQk9VKwxpq+woO8vdrwBbz7kPMHp+v+ACxaf88M9DpTVctvvP6XmRDMv3j2LacPje6hYY0xfYkHf13Vj2qSnfUerue25TTS7lFVLZjNpWFwPFGuM6Yss6Puybkyb9LSz+Dh3/GETQUHCy0tmMzY1pgeKNcb0VRb0fVFzI/ztl/Dx012aNulpa0El31n+GVFhway8exYjk72fmWOMCQwW9H1NyW547W7n4R9dmDbpKedgBYuf30z8oFBeuns2wxOifFysMaY/sKDvK3wwbdLTxn2lfP+FLaTGRvDSPbMYGhfpw2KNMf2JBX1f4DltcszVsOC35zxt0tOfcwp4/LXtjE6J5k/fm0lKbIQPizXG9DcW9P6243V48+FuTZtspar85v18/uO9vcwZncizt19AbMS5f3lrjAksFvT+4qNpk62aWlz899fzeCWngJump/HkTVmEhQT5sGBjTH9lQe8Pp02bfBQu/UmXpk22qjnRzP0rP+fDvaU8cMVofnTVWHv0nzHmJAv63uTDaZOtSo43sHjFZnYfqeZ/3TSFRTPtgSHGmNNZ0PeWkl3u1Sa7P22yVX5JNd9dvpljdY08951svjbeVqA0xpzJgr6ntU6bfPdfIDym29MmW23aX849f8ohLCSYV5ZcyJR0W9LAGNM+C/qe5ONpk63e2lbMj17ZSnpCJH9cPNNuhDLGnJUFfU/x4bTJVqrKcxsP8Mv1u5iRMZjffyeb+KgwHxVsjAlUFvS+1nzCWU7YR9MmW7W4lJ+/tZMVnxzkuilD+fdbphIRGuyDgo0xgc6C3pdcLc4XrjvXwqU/hct+2q1pk60amlp46OUv2LDjKHdfnMnPrp1AUJBNnzTGeMeC3ldU4e0fOSF/9b/Bhff75LIVtY3c9cfN5BZU8sS8iSyek+mT6xpjBg6vbp0UkbkiskdE8kXksXaOx4nImyKyVUR2iMhib88NGP/1c9iyAi7+kc9C/qvyWm5+9hN2Fh/n2dumW8gbY7qk0x69iAQDzwBXAYXAZhFZp6o7PZrdD+xU1XkikgzsEZGVQIsX5/Z//3gGNv67Mz/+yn/xySVzCyq5a8VmXKq8dM8sLjjPnu1qjOkab3r0M4F8Vd2vqo3Ay8CCNm0UiBHnvvtooAJo9vLc/i13FWz4GUyY78yu8cHSA+/uPMrCZf9gUHgIr953kYW8MaZbvAn6NKDAY7vQvc/Tb4EJQDGwHXhIVV1entt/7fkLrL0fMi+Dm5+DoO7PgnnhHwf5/gs5jEuN4bUfXGRPhDLGdJs3X8a210XVNttXA7nAFcAo4F0R2ejluc6LiCwBlgCMGNEP1mv56hP4850wNAsWroSQ8G5dzuVS/s+GPSz98Eu+PiGF3yw6n6gw+67cGNN93vToC4HhHtvpOD13T4uB19SRDxwAxnt5LgCqukxVs1U1Ozk52dv6/ePwNnjp2xA3HG571VnaoBtONLfw8Cu5LP3wS26fPYKlt19gIW+M8Rlv0mQzMEZEMoEiYCFwa5s2h4ArgY0ikgqMA/YDlV6c279U7IcXb4bwWPjOGzAosVuXq6pv4vsv5PDp/gp+Oncc9102ypYYNsb4VKdBr6rNIvJDYAMQDCxX1R0icq/7+FLg58AKEdmOM1zzqKqWAbR3bs+8lV5QfQT+dAO4muHOtyEuvVuXK6qs587ln3GwvJanF05jwbTA+frCGNN3iGq7Q+Z+lZ2drTk5Of4u43T1x+D56+DYQbjzTWd5g27YUVzF4uc3U9/Uwn/ecQEXjUryTZ3GmAFJRLaoanZ7x2wg2BuNdc6YfPk+uHV1t0P+o72l3PfiFmIjQ1lz70WMG9K9MX5jjDkbC/rOtDTB6u9AwWfwrRUw6mvdutyfcwp4/LXtjE6JZsXimQyJi/BNncYY0wEL+rNxuZz15PPfhet/DZNu6PKlVJXfvJ/Pf7y3l4tHJ/Hs7dOJiej+gmfGGNMZC/qOqMJfH4Ptq51lDbIXd35OB5paXPz31/N4JaeAm6an8eRNWYSFeLXMkDHGdJsFfUc++hV89p8w+35nobIuqjnRzP0rP+fDvaU8eMVoHrlqrE2fNMb0Kgv69mx+Dv72S5i6CL7xiy6vX1NyvIHFKzaz+0g1T940hYUz+8Edv8aYgGNB31beq/D2P8HYa2D+/4Ogrg2x5JdU893lmzlW18hz383ma+O6/6xYY4zpCgt6T/nvw2vfhxEXwree7/LToTbtL+eeP+UQFhLMK0suZEp6nI8LNcYY71nQtyrYDK/cDsnjYdEqCI3s0mUKj9Vxx/LPGD44khWLZzI8IcrHhRpjzLmxqR8AJbvgpW9BdCrc/ipExnf5Umtzi2lsdvH8nRbyxpi+wYK+8hC8cBMEhzuLlMWkduty63KLueC8wYxItJA3xvQNAzvoa0rhhRuhqRbueA0GZ3TrcruPHGfP0WoWTBvmm/qMMcYHBu4YfcNxWHkzVBU5PfnUSd2+5NrcYoKDhGunDPVBgcYY4xsDM+ibGuDlW+HoDli4CkbM7vYlXS5lXW4xF49OIim6e0+bMsYYXxp4QzctzfDqXXBwI9zwLIz9hk8u+/mhYxRV1tuwjTGmzxlYQa8Kbz0Eu9+Cuf8bsm7x2aXX5hYTHhLENyYN8dk1jTHGFwZW0L/3BHzxIlz2KMy+12eXbWpx8fb2w3x9YirR4QNzNMwY03cNnKD/+Gnnz4y74fLHfXrpv+eXUVHbyIKpNmxjjOl7Bn89YggAAA9NSURBVEbQf/4CvPsvMOkmuOb/dHmRso68mVtMbEQIl41L9ul1jTHGFwI/6He9BW8+CKOugBv/E4KCfXr5+sYWNuw4wrVThhIe4ttrG2OMLwR20B/YCGu+B8Omw7dfhJAwn7/E+7uPUtvYwnybbWOM6aO8CnoRmSsie0QkX0Qea+f4T0Qk1/0nT0RaRCTBfewREdnh3r9KRHrnIanFubBqESRkwm1/hrBBPfIya3OLSY0NZ1ZmYo9c3xhjuqvToBeRYOAZ4BpgIrBIRCZ6tlHVX6nqNFWdBjwOfKiqFSKSBjwIZKvqZCAYWOjrN3GGsnx48WaIHAx3vA5RCT3yMlV1TXywp4R5WcMIDrKnRhlj+iZvevQzgXxV3a+qjcDLwIKztF8ErPLYDgEiRSQEiAKKu1qsV44XO+vXgBPysT03pPKXvMM0tSgLpqX12GsYY0x3eRP0aUCBx3ahe98ZRCQKmAu8CqCqRcBTwCHgMFClqu90cO4SEckRkZzS0lLv34Gnugon5OuPOcsNJ43u2nW8tDa3mJFJg5icFtujr2OMMd3hTdC3NyahHbSdB3ysqhUAIjIYp/efCQwDBonI7e2dqKrLVDVbVbOTk7swTbGxFl66BSoOOA8OGTbt3K9xDo5UNfDpgXLmTxtmD/s2xvRp3gR9ITDcYzudjodfFnL6sM3XgQOqWqqqTcBrwEVdKbRTEgwxQ+GbyyHzkh55CU9vbStGFebbTVLGmD7Om/v1NwNjRCQTKMIJ81vbNhKROOAywLPHfgiY7R7SqQeuBHK6W3S7QiPglj/5/GaojqzNLSYrPY6RydG98nrGGNNVnfboVbUZ+CGwAdgFrFbVHSJyr4h4LhhzI/COqtZ6nLsJWAN8Dmx3v94yH9Z/ul4K+S9La9heVGW9eWNMv+DVClyquh5Y32bf0jbbK4AV7Zz7BPBElyvsg9blFiMC8yzojTH9QGDfGdsDVJV1W4u5cGQiqbG9c++XMcZ0hwX9OdpeVMWBslp7wIgxpt+woD9Ha3OLCQ0W5k6y58IaY/oHC/pz0OJS3txazOXjUoiLCvV3OcYY4xUL+nOwaX85JdUnbNjGGNOvWNCfg7W5xQwKC+bK8an+LsUYY7xmQe+lE80trM87zNWThhAZZg8YMcb0Hxb0XvpgTynVDc32gBFjTL9jQe+ldbnFJA4KY87oJH+XYowx58SC3gvVDU28t+so12UNJTTYPjJjTP9iqeWFd3Yc5USzy2bbGGP6JQt6L6zdWkz64Eimjxjs71KMMeacWdB3orT6BB/nlzF/qj1gxBjTP1nQd2L99sO0uOy5sMaY/suCvhNrc4sYPySGcUNi/F2KMcZ0iQX9WRwqr+PzQ5U2d94Y069Z0J/Fm9ucR+POy7KgN8b0Xxb0HVBV3viiiOzzBjM8Icrf5RhjTJdZ0Hdg95Fq9pXU2Nx5Y0y/Z0HfgbW5xQQHCddOsQeMGGP6N6+CXkTmisgeEckXkcfaOf4TEcl1/8kTkRYRSXAfixeRNSKyW0R2iciFvn4TvuZyP2DkkjFJJEaH+7scY4zplk6DXkSCgWeAa4CJwCIRmejZRlV/parTVHUa8DjwoapWuA8/DfxVVccDU4FdvnwDPeHzQ8coqqy3YRtjTEDwpkc/E8hX1f2q2gi8DCw4S/tFwCoAEYkFLgX+AKCqjapa2b2Se97a3GIiQoO4auIQf5dijDHd5k3QpwEFHtuF7n1nEJEoYC7wqnvXSKAUeF5EvhCR50RkUDfq7XFNLS7e3n6Yr09IJTo8xN/lGGNMt3kT9O0t8KIdtJ0HfOwxbBMCTAeeVdXzgVrgjDF+ABFZIiI5IpJTWlrqRVk94+/5ZVTUNtqSB8aYgOFN0BcCwz2204HiDtouxD1s43Fuoapucm+vwQn+M6jqMlXNVtXs5ORkL8rqGetyi4mLDOWysf6rwRhjfMmboN8MjBGRTBEJwwnzdW0biUgccBmwtnWfqh4BCkRknHvXlcDOblfdQ+obW9iw4wjXThlCWIjNPDXGBIZOB6FVtVlEfghsAIKB5aq6Q0TudR9f6m56I/COqta2ucQDwEr3/yT2A4t9Vr2PvbfrKHWNLcyfasM2xpjA4dW3jaq6HljfZt/SNtsrgBXtnJsLZHe5wl60NreYIbERzMxM8HcpxhjjMzY+4VZZ18iHe0uYN3UowUH2gBFjTOCwoHf7S94RmlrsASPGmMBjQe+2NreIkcmDmDQs1t+lGGOMT1nQA4er6tl0oMKeC2uMCUgW9MBbWw+jCvOn2to2xpjAY0EPrN1aRFZ6HCOTo/1dijHG+NyAD/r8khryio5bb94YE7AGfNCv21qMCMyzoDfGBKgBHfSqyrrcIi4cmUhqbIS/yzHGmB4xoIN+W2EVB8vr7AEjxpiANqCDfm1uMWHBQcydZM+FNcYErgEb9C0u5c1txVw+Lpm4qFB/l2OMMT1mwAb9p/vLKa0+YUseGGMC3oAN+rW5RQwKC+bKCSn+LsUYY3rUgAz6hqYW/pJ3hKsnDyEiNNjf5RhjTI8akEH/wZ5SqhuabdjGGDMgDMigX7e1iMRBYcwZlejvUowxpscNuKCvbmjivV0lXJ81lJDgAff2jTED0IBLug07jtLY7GK+DdsYYwaIARf0a3OLSB8cyfQR8f4uxRhjesWACvrS6hN8nF/Ggmn2gBFjzMDhVdCLyFwR2SMi+SLyWDvHfyIiue4/eSLSIiIJHseDReQLEXnLl8Wfq7e3FeNSbLaNMWZA6TToRSQYeAa4BpgILBKRiZ5tVPVXqjpNVacBjwMfqmqFR5OHgF2+K7tr1m4tZvyQGMamxvi7FGOM6TXe9OhnAvmqul9VG4GXgQVnab8IWNW6ISLpwHXAc90ptLsOldfxxaFK680bYwYcb4I+DSjw2C507zuDiEQBc4FXPXb/Gvgp4Drbi4jIEhHJEZGc0tJSL8o6N29uKwZg3lRbqdIYM7B4E/TtfWupHbSdB3zcOmwjItcDJaq6pbMXUdVlqpqtqtnJyclelOU9VeWNL4qYkTGY9MFRPr22Mcb0dd4EfSEw3GM7HSjuoO1CPIZtgDnAfBE5iDPkc4WIvNiFOrtl95Fq9pXU2Nx5Y8yA5E3QbwbGiEimiIThhPm6to1EJA64DFjbuk9VH1fVdFXNcJ/3X6p6u08qPwdrc4sJCRKum2LDNsaYgSekswaq2iwiPwQ2AMHAclXdISL3uo8vdTe9EXhHVWt7rNoucLmUN7cWc8mYJBIGhfm7HGOM6XWdBj2Aqq4H1rfZt7TN9gpgxVmu8QHwwTnW121bDh2jqLKen1w9rrdf2hhj+oSAvzN2bW4REaFBXDUx1d+lGGOMXwR00De1uHh722GumjiEQeFe/ePFGGMCTkAH/d/3lXGsrokFU4f5uxRjjPGbgA76tblFxEWGculY387LN8aY/iRgg76usZl3dh7l2ilDCAsJ2LdpjDGdCtgEfG9XCXWNLcyfajdJGWMGtoAN+nW5RQyJjWBmZkLnjY0xJoAFZNAfq23kgz2lzJs6lOAge8CIMWZgC8ig/0veEZpdaksSG2MMARr0a3OLGJk8iEnDYv1dijHG+F3ABX1xZT2fHaxgwdQ0ey6sMcYQgEH/1rZiVGH+NLtJyhhjIACDfm1uMVPT48hMGuTvUowxpk8IqKDPL6lmR/Fxe8CIMcZ4CKigX5dbjAjMy7IHjBhjTKuACXpVZe3WYi4alUhKbIS/yzHGmD4jYNburW9q4cKRiVw0OsnfpRhjTJ8SMEEfFRbCkzdn+bsMY4zpcwJm6MYYY0z7LOiNMSbAWdAbY0yA8yroRWSuiOwRkXwReayd4z8RkVz3nzwRaRGRBBEZLiJ/E5FdIrJDRB7y/VswxhhzNp0GvYgEA88A1wATgUUiMtGzjar+SlWnqeo04HHgQ1WtAJqBH6vqBGA2cH/bc40xxvQsb3r0M4F8Vd2vqo3Ay8CCs7RfBKwCUNXDqvq5+/dqYBdgt60aY0wv8ibo04ACj+1COghrEYkC5gKvtnMsAzgf2NTBuUtEJEdEckpLS70oyxhjjDe8Cfr21vrVDtrOAz52D9ucuoBINE74P6yqx9s7UVWXqWq2qmYnJyd7UZYxxhhveHPDVCEw3GM7HSjuoO1C3MM2rUQkFCfkV6rqa94UtWXLljIR+cqbtn1YElDm7yL6CPssTmefx+ns8zilO5/FeR0dENWOOufuBiIhwF7gSqAI2Azcqqo72rSLAw4Aw1W11r1PgD8CFar6cBeL75dEJEdVs/1dR19gn8Xp7PM4nX0ep/TUZ9Hp0I2qNgM/BDbgfJm6WlV3iMi9InKvR9MbgXdaQ95tDnAHcIXH9MtrfVi/McaYTni11o2qrgfWt9m3tM32CmBFm31/p/0xfmOMMb3E7oztOcv8XUAfYp/F6ezzOJ19Hqf0yGfR6Ri9McaY/s169MYYE+As6I0xJsBZ0PuQLeJ2JhEJFpEvROQtf9fibyISLyJrRGS3+7+RC/1dkz+JyCPuvyd5IrJKRAbUM0BFZLmIlIhInse+BBF5V0T2uX8O9sVrWdD7li3idqaHcKblGnga+KuqjgemMoA/FxFJAx4EslV1MhCMc8PlQLICZ8kYT48B76vqGOB993a3WdD7kC3idjoRSQeuA57zdy3+JiKxwKXAHwBUtVFVK/1bld+FAJHumzKj6PiO+4Ckqh8BFW12L8C5yRT3zxt88VoW9D2ks0XcBohfAz8FXP4upA8YCZQCz7uHsp4TkUH+LspfVLUIeAo4BBwGqlT1Hf9W1SekquphcDqOQIovLmpB3wO8WcQt0InI9UCJqm7xdy19RAgwHXhWVc8HavHRP8v7I/fY8wIgExgGDBKR2/1bVeCyoPexriziFqDmAPNF5CDOMwyuEJEX/VuSXxUChara+i+8NTjBP1B9HTigqqWq2gS8Blzk55r6gqMiMhTA/bPEFxe1oPch9yJufwB2qer/9Xc9/qSqj6tquqpm4HzJ9l+qOmB7bKp6BCgQkXHuXVcCO/1Ykr8dAmaLSJT7782VDOAvpz2sA77r/v27wFpfXNSrtW6M11oXcdsuIrnufT9zrxVkzAPAShEJA/YDi/1cj9+o6iYRWQN8jjNb7QsG2FIIIrIKuBxIEpFC4AngSWC1iNyF8z/Db/nktWwJBGOMCWw2dGOMMQHOgt4YYwKcBb0xxgQ4C3pjjAlwFvTGGBPgLOiNMSbAWdAbY0yA+/+V40RFCWHoAwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=hist.history['val_accuracy'], label='valid')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbwn0ekDy_s2"
      },
      "source": [
        "### 5 - Inferencia\n",
        "Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_sentence(input_seq):\n",
        "    # Se transforma la sequencia de entrada a los stados \"h\" y \"c\" de la LSTM\n",
        "    # para enviar la primera vez al decoder\"\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "    # Se obtiene el indice que finaliza la inferencia\n",
        "    eos = word2idx_outputs['<eos>']\n",
        "    \n",
        "    output_sentence = []\n",
        "    for _ in range(max_out_len):\n",
        "        # Predicción del próximo elemento\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "        # Si es \"end of sentece <eos>\" se acaba\n",
        "        if eos == idx:\n",
        "            break\n",
        "\n",
        "        # Transformar ídx a palabra\n",
        "        word = ''        \n",
        "        if idx > 0:\n",
        "            word = idx2word_target[idx]\n",
        "            output_sentence.append(word)\n",
        "\n",
        "        # Actualizar los estados dado la ultimo prediccion\n",
        "        states_value = [h, c]\n",
        "\n",
        "        # Actualizar secuencia de entrada con la salida (re-alimentacion)\n",
        "        target_seq[0, 0] = idx\n",
        "\n",
        "    return ' '.join(output_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: Hello\n",
            "Representacion en vector de tokens de ids [19]\n",
            "Padding del vector: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19]]\n",
            "Input: Hello\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "Response: hello how are you\n"
          ]
        }
      ],
      "source": [
        "input_test = \"Hello\"\n",
        "print('Input:', input_test)\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "print(\"Representacion en vector de tokens de ids\", integer_seq_test)\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "print(\"Padding del vector:\", encoder_sequence_test)\n",
        "\n",
        "print('Input:', input_test)\n",
        "translation = translate_sentence(encoder_sequence_test)\n",
        "print('Response:', translation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "6d - bot_qa.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e7d167b8bde7d7cf9a172b3e3477586a36da8945a7654fbe033faf78f7d94a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
