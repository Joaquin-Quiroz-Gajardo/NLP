{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA7nqkumo9z9"
      },
      "source": [
        "### Objetivo\n",
        "Se evaluara la similitud entre palabras a partir de la versión protestante de la biblia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lFToQs5FK5uZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g07zJxG7H9vG"
      },
      "source": [
        "### Datos\n",
        "Utilizaremos como dataset canciones de bandas de habla inglés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ticoqYD1Z3I7"
      },
      "outputs": [],
      "source": [
        "# Armar el dataset utilizando salto de línea para separar las oraciones/docs\n",
        "from codecs import ignore_errors\n",
        "import pandas as pd\n",
        "f = open(\"pg10.txt\", \"r\")\n",
        "documents = pd.DataFrame({\"document\":[]})\n",
        "for x in f:\n",
        "    if x[0] not in \"0123456789\" or len(x.split(' ')) < 2:\n",
        "        continue\n",
        "    # print(x.split(' ', 1)[1])\n",
        "    df2 = pd.DataFrame({\"document\":[x.split(' ', 1)[1]]})\n",
        "    documents = documents.append(df2, ignore_index = True)\n",
        "df = documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LEpKubK9XzXN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de documentos: 25017\n"
          ]
        }
      ],
      "source": [
        "print(\"Cantidad de documentos:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab94qaFlrA1G"
      },
      "source": [
        "### 1 - Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rIsmMWmjrDHd"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "sentence_tokens = []\n",
        "# Recorrer todas las filas y transformar las oraciones\n",
        "# en una secuencia de palabras (esto podría realizarse con NLTK o spaCy también)\n",
        "for _, row in df[:None].iterrows():\n",
        "    sentence_tokens.append(text_to_word_sequence(row[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CHepi_DGrbhq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['in',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'god',\n",
              " 'created',\n",
              " 'the',\n",
              " 'heavens',\n",
              " 'and',\n",
              " 'the',\n",
              " 'earth']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Demos un vistazo\n",
        "sentence_tokens[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaXV6nlHr5Aa"
      },
      "source": [
        "### 2 - Crear los vectores (word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OSb0v7h8r7hK"
      },
      "outputs": [],
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
        "# Sobracargamos el callback para poder tener esta información\n",
        "class callback(CallbackAny2Vec):\n",
        "    \"\"\"\n",
        "    Callback to print loss after each epoch\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "        if self.epoch == 0:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "        else:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "        self.epoch += 1\n",
        "        self.loss_previous_step = loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "i0wnDdv9sJ47"
      },
      "outputs": [],
      "source": [
        "# Crearmos el modelo generador de vectoeres\n",
        "# En este caso utilizaremos la estructura modelo Skipgram\n",
        "w2v_model = Word2Vec(min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
        "                     window=2,       # cant de palabras antes y desp de la predicha\n",
        "                     size=1000,       # dimensionalidad de los vectores \n",
        "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
        "                     workers=1,      # si tienen más cores pueden cambiar este valor\n",
        "                     sg=1)           # modelo 0:CBOW  1:skipgram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5lTt8wErsf17"
      },
      "outputs": [],
      "source": [
        "# Buildear el vocabularui con los tokens\n",
        "w2v_model.build_vocab(sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TNc9qt4os5AT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de docs en el corpus: 25017\n"
          ]
        }
      ],
      "source": [
        "# Cantidad de filas/docs encontradas en el corpus\n",
        "print(\"Cantidad de docs en el corpus:\", w2v_model.corpus_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "idw9cHF3tSMl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de words distintas en el corpus: 3122\n"
          ]
        }
      ],
      "source": [
        "# Cantidad de words encontradas en el corpus\n",
        "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model.wv.vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC9mZ8DPk-UC"
      },
      "source": [
        "### 3 - Entrenar el modelo generador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QSp-x0PAsq56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after epoch 0: 1897250.0\n",
            "Loss after epoch 1: 1406046.25\n",
            "Loss after epoch 2: 1324239.25\n",
            "Loss after epoch 3: 1251913.0\n",
            "Loss after epoch 4: 1235473.0\n",
            "Loss after epoch 5: 1220400.5\n",
            "Loss after epoch 6: 1156007.0\n",
            "Loss after epoch 7: 1143543.0\n",
            "Loss after epoch 8: 1135640.0\n",
            "Loss after epoch 9: 1129627.0\n",
            "Loss after epoch 10: 1120744.0\n",
            "Loss after epoch 11: 1117670.0\n",
            "Loss after epoch 12: 1113296.0\n",
            "Loss after epoch 13: 1083451.0\n",
            "Loss after epoch 14: 1057756.0\n",
            "Loss after epoch 15: 1054300.0\n",
            "Loss after epoch 16: 1055294.0\n",
            "Loss after epoch 17: 1053974.0\n",
            "Loss after epoch 18: 1061320.0\n",
            "Loss after epoch 19: 1068800.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(3783978, 6139340)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Entrenamos el modelo generador de vectores\n",
        "# Utilizamos nuestro callback\n",
        "w2v_model.train(sentence_tokens,\n",
        "                 total_examples=w2v_model.corpus_count,\n",
        "                 epochs=20,\n",
        "                 compute_loss = True,\n",
        "                 callbacks=[callback()]\n",
        "                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddT9NVuNlCAe"
      },
      "source": [
        "### 4 - Ensayar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6cHN9xGLuPEm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('whirlwind', 0.6389119625091553),\n",
              " ('visions', 0.6316908001899719),\n",
              " ('messenger', 0.5773029923439026),\n",
              " ('elisha', 0.5770173072814941),\n",
              " ('row', 0.5575631856918335),\n",
              " ('samson', 0.5525782704353333),\n",
              " ('gehazi', 0.5453565716743469),\n",
              " ('hour', 0.5430376529693604),\n",
              " ('pharisee', 0.5419018268585205),\n",
              " ('philip', 0.5412166118621826)]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Palabras que MÁS se relacionan con...:\n",
        "w2v_model.wv.most_similar(positive=[\"angel\"], topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "47HiU5gdkdMq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('commit', 0.03606146574020386),\n",
              " ('thyself', 0.019832352176308632),\n",
              " ('lest', 0.011996285058557987),\n",
              " ('sin', -0.0013654939830303192),\n",
              " (\"man's\", -0.00898822769522667),\n",
              " ('wise', -0.016552120447158813),\n",
              " ('shed', -0.01953708752989769),\n",
              " ('precious', -0.022870268672704697),\n",
              " ('own', -0.024061331525444984),\n",
              " ('their', -0.02619529329240322)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Palabras que MENOS se relacionan con...:\n",
        "w2v_model.wv.most_similar(negative=[\"angel\"], topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DT4Rvno2mD65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('firmament', 0.5945953130722046),\n",
              " ('lightnings', 0.5425839424133301),\n",
              " ('heavens', 0.5368310809135437),\n",
              " ('throne', 0.5239861011505127),\n",
              " ('ascend', 0.5195358991622925),\n",
              " ('falling', 0.5088451504707336),\n",
              " ('descended', 0.5055993795394897),\n",
              " ('lights', 0.4988625645637512),\n",
              " ('wonder', 0.498734712600708),\n",
              " ('carmel', 0.49806803464889526)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Palabras que MÁS se relacionan con...:\n",
        "w2v_model.wv.most_similar(positive=[\"heaven\"], topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XPLDPgzBmQXt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('hosts', 0.6238108277320862),\n",
              " ('manoah', 0.6138322353363037),\n",
              " ('praises', 0.5962761640548706),\n",
              " ('alas', 0.592499852180481),\n",
              " ('sir', 0.5865200757980347),\n",
              " ('god', 0.5809298157691956),\n",
              " ('steward', 0.5782966017723083),\n",
              " ('almighty', 0.5749210119247437),\n",
              " ('exhortation', 0.5742908716201782),\n",
              " ('vengeance', 0.5652214884757996)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Palabras que MÁS se relacionan con...:\n",
        "w2v_model.wv.most_similar(positive=[\"lord\"], topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g8UVWe6lFmh"
      },
      "source": [
        "### 5 - Visualizar agrupación de vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDxEVXAivjr9"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import IncrementalPCA    \n",
        "from sklearn.manifold import TSNE                   \n",
        "import numpy as np                                  \n",
        "\n",
        "def reduce_dimensions(model):\n",
        "    num_dimensions = 2  \n",
        "\n",
        "    vectors = np.asarray(model.wv.vectors)\n",
        "    labels = np.asarray(model.wv.index2word)  \n",
        "\n",
        "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    x_vals = [v[0] for v in vectors]\n",
        "    y_vals = [v[1] for v in vectors]\n",
        "    return x_vals, y_vals, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCCXtDpcugmd"
      },
      "outputs": [],
      "source": [
        "# Graficar los embedddings en 2D\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "x_vals, y_vals, labels = reduce_dimensions(w2v_model)\n",
        "\n",
        "MAX_WORDS=200\n",
        "fig = px.scatter(x=x_vals[:MAX_WORDS], y=y_vals[:MAX_WORDS], text=labels[:MAX_WORDS])\n",
        "fig.show(renderer=\"colab\") # esto para plotly en colab"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3b - Custom embedding con Gensim",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e7d167b8bde7d7cf9a172b3e3477586a36da8945a7654fbe033faf78f7d94a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
